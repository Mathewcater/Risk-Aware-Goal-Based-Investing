{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanatory Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notebook for **Risk-Aware Reinforcement Learning for Goal-Based Portfolio Optimisation**, by Mathew Cater Benavides and Sebastian Jaimungal*,\n",
    "\n",
    "This Jupyter notebook showcases how to use the Python files for using our proposed deep policy gradient algorithm and reproducing some of the main figures in the experiments section. All the Python code and modules are publicly available in the Github repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importation of Python Libraries\n",
    "\n",
    "We load some basic Python libraries, such as NumPy, PyTorch, SciPy and Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# pytorch\n",
    "import torch as T\n",
    "\n",
    "# misc.\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from torch.nn import ReLU\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Palatino\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We as well load the relevant classes and functions in our Python modules and refer the user to the Python files for a description of all functions in the modules. Furthermore, the user can modify these files to consider non-investigated market models or risk measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cmap, colors # colors\n",
    "from hyperparams import init_params, print_params # hyperparameters\n",
    "from models import PolicyANN # ANN structures\n",
    "from envs import BS_Environment, FactorModel_Environment # environment\n",
    "from agents import Agent # deep policy gradient algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialisation of Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_model = \"BS\" # specify the market model\n",
    "num_assets = 4\n",
    "env_params = {'num_assets': num_assets, # number of total assets in market (risk-free and risky)\n",
    "                'drifts': T.tensor([0.03, 0.06, 0.09]), # drifts of risky assets\n",
    "                'vols': T.tensor([0.06, 0.12, 0.18]), # volatilities of risky assets\n",
    "                'corr_matrix': T.tensor([[1.0, 0.2, 0.2], # correlation structure of risky asset prices\n",
    "                                         [0.2, 1.0, 0.2],\n",
    "                                         [0.2, 0.2, 1.0]]), \n",
    "                'interest_rate': 0.025, # interest rate of risk-free asset                \n",
    "                'goal_prob': 0.0, # confidence level of meeting returns requirement \n",
    "                'returns_req': 0.0, # returns requirement for goal\n",
    "                'alpha': 0.80,\n",
    "                'beta': 0.9, \n",
    "                'q': 1.0, \n",
    "                'phi': 0.0, # transaction costs\n",
    "                'T': 1.0, # trading horizon\n",
    "                'Ndt': 1, # number of periods\n",
    "                'init_wealth': 1.0, # initial wealth\n",
    "                'S0': T.tensor([1.0, 1.0, 1.0]) # initial risky asset prices\n",
    "                }\n",
    "algo_params = {'num_epochs' : 1_000, # number of iterations of entire training loop\n",
    "                'batch_size' : 750, # mini-batch size for gradient estimates\n",
    "                'num_layers': 10, # number of layer in policy network\n",
    "                'hidden_size': 16, # width of hidden layers of policy network\n",
    "                'learn_rate': 0.001, # learning rate of policy network\n",
    "                'init_lamb' : 1.0, # initial Lagrange multiplier    \n",
    "                'init_mu' : 10.0, # initial penalty strength\n",
    "                'pen_strength_lr' : 1.5, # penalty strength learning rate\n",
    "                'pen_update_freq' : 5, # Lagrange multiplier and penalty strength update frequency\n",
    "                } \n",
    "print_params(env_params, algo_params, market_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the environment from the envs.py file to get functions to interact with the data-generating processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BS_Environment(env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training of Risk-Aware Agents & Initialisation of Testing Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the neural network structure for our algorithm from the **models.py** file, we as well instantiate our risk-aware agent from the *agents.py* file with the aforementioned policy, environment and algorithm specifications. The $\\texttt{Agent()}$ class regroups the different routines to solve our optimisation problem. We have respectively:\n",
    "\n",
    "- an ANN representing the policy $\\pi^{\\theta}$;\n",
    "- an $\\texttt{Agent()}$ object representing the risk-aware agent with the aforementioned specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyANN(env, algo_params)\n",
    "agent = Agent(env=env, # environment\n",
    "              algo_params=algo_params, # algorithm specifications \n",
    "              policy=policy) # policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next start the training phase of our deep policy gradient algorithm -- this corresponds to the Algorithm 5.1 presented in our paper. We as well set up hyperparameters for the testing phase, in particular, the number of episodes and the seed for the random number generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.random.manual_seed(54321) # set seed for replication purposes\n",
    "Nsimulations = 10_000 # number of simulations following the optimal strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time the algorithm\n",
    "start_time = time()\n",
    "\n",
    "for epoch in tqdm(range(agent.algo_params[\"num_epochs\"])):\n",
    "            \n",
    "    # at final epoch of training, store prob. of meeting goal, dist. of term. wealth and action hist. over final mini-batch\n",
    "    if epoch == agent.algo_params[\"num_epochs\"] - 1:\n",
    "        agent.algo_params[\"batch_size\"] = Nsimulations\n",
    "        \n",
    "        # update policy     \n",
    "        _, RDEU, return_prob, term_wealth_samps, pi = agent.update_policy() \n",
    "        \n",
    "        # store RDEU to track training\n",
    "        agent.update_history(RDEU, return_prob) \n",
    "        \n",
    "        # store store prob. of meeting goal, dist. of term. wealth and action hist.\n",
    "        agent.term_wealth_dist = term_wealth_samps \n",
    "        agent.return_prob = return_prob\n",
    "        agent.position_hist = pi\n",
    "    \n",
    "    else:\n",
    "        # update policy     \n",
    "        _, _, return_prob, term_wealth_samps, pi = agent.update_policy() \n",
    "        \n",
    "        # update Lagrange multipliers\n",
    "        if epoch % agent.algo_params[\"pen_update_freq\"] == 0:\n",
    "            agent.update_multipliers(constr_err=ReLU()(agent.env.params[\"goal_prob\"] - return_prob))    \n",
    "            \n",
    "    start_time = time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing of Risk-Aware Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Distribution of Terminal Wealth\n",
    "\n",
    "We investigate the terminal wealth distribution when the agent adheres to the learned optimal policy. To accomplish this, we execute a large batch of complete episodes in accordance with the learned policy. We as well report the probability that the agent achieves their desired realised gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 3, figsize=(18,5))  \\\n",
    "                            \n",
    "# plot RDEU through training\n",
    "ax[0].plot(np.arange(algo_params[\"num_epochs\"]) + 1, \n",
    "                     T.stack(agent.RDEU_history)[:-1], \n",
    "                     color=colors[0])\n",
    "ax[0].set(xlabel='Epochs', ylabel='RDEU', title='RDEU by Epoch')    \n",
    "ax[0].grid()\n",
    "\n",
    "# plot terminal wealth distribution of learned strategy using Nsims full episodes\n",
    "term_wealth = agent.term_wealth_dist.detach().squeeze()\n",
    "mean_term_wealth, std_term_wealth = T.mean(term_wealth), T.std(term_wealth)\n",
    "num_bins = 50\n",
    "domain = np.linspace(mean_term_wealth - 5*std_term_wealth, mean_term_wealth + 5*std_term_wealth, 1500)\n",
    "term_wealth_kde = gaussian_kde(term_wealth)\n",
    "ax[1].hist(term_wealth, density=True, bins=num_bins, color=colors[0], alpha=0.5)\n",
    "ax[1].set(title=r'Distribution of Terminal Wealth', xlabel=r'Terminal Wealth: $X^{\\theta}$', ylabel='Density')\n",
    "ax[1].plot(domain, term_wealth_kde(domain), color=colors[0])\n",
    "\n",
    "# plot probability of posting desired return through training\n",
    "ax[2].plot(np.arange(algo_params[\"num_epochs\"]) + 1,\n",
    "                     T.stack(agent.constraint_prob_history)[:-1],\n",
    "                     color=colors[0])\n",
    "ax[2].set(xlabel='Epochs', ylabel=r'$\\mathbb{P}(X^\\theta \\geq (1+c)X_0)$', title='Probability of Success by Epoch')    \n",
    "ax[2].grid()\n",
    "\n",
    "# check if constraint satisfied\n",
    "c = env_params[\"returns_req\"]\n",
    "print(f'Probability of return exceeding {100*c}%: {str(round(float(100*agent.return_prob), 3))}%')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Strategy Analysis\n",
    "\n",
    "We observe the position paths in each of the assets over the 10,000 full testing episodes and plot their quantiles to investigate the learned strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hist = agent.position_hist.detach().squeeze().numpy()\n",
    "\n",
    "if env_params['Ndt'] == 1:\n",
    "    \n",
    "    # figure parameters\n",
    "    nrows = 1\n",
    "    ncols = num_assets\n",
    "    fig, axes = plt.subplots(nrows, ncols, sharey='all', sharex='all', figsize=(15,7))\n",
    "    grid = plt.GridSpec(nrows, ncols)\n",
    "    labels = [r'$\\pi_{\\theta}^{(0)}$', r'$\\pi_{\\theta}^{(1)}$', r'$\\pi_{\\theta}^{(2)}$', r'$\\pi_{\\theta}^{(3)}$']\n",
    "\n",
    "    for idx_asset in range(num_assets):\n",
    "        temp = axes[idx_asset].imshow(position_hist[0][idx_asset].reshape(1,1),\n",
    "                                interpolation='none',\n",
    "                                cmap=cmap,\n",
    "                                aspect='auto',\n",
    "                                vmin=np.array(0.0),\n",
    "                                vmax=np.array(1.0)\n",
    "                                )\n",
    "        axes[idx_asset].set_xticks([])\n",
    "        axes[idx_asset].set_yticks([])\n",
    "        axes[idx_asset].set_title(f'Allocation Asset {idx_asset}: {labels[idx_asset]}', fontsize=15)\n",
    "\n",
    "    fig.colorbar(temp, orientation='horizontal', ax=axes, shrink=0.8, pad=0.2)\n",
    "    fig.suptitle('Learned Allocations', fontsize=19)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, env_params[\"num_assets\"], figsize=(18,5), sharey=True)\n",
    "    position_hist = agent.position_hist.detach().squeeze()\n",
    "    periods = T.arange(env_params[\"Ndt\"])\n",
    "    # quantile \n",
    "    q = 0.5\n",
    "\n",
    "    #labels\n",
    "    labels = [r'$\\pi_{\\theta}^{(0)}$', r'$\\pi_{\\theta}^{(1)}$', r'$\\pi_{\\theta}^{(2)}$', r'$\\pi_{\\theta}^{(3)}$']\n",
    "\n",
    "    for j in range(env.params[\"num_assets\"]):\n",
    "        axs[j].plot(np.quantile(position_hist[:,:,j], axis=0, q=q), color=colors[0], label=f'q={q}') # plot quantile\n",
    "        axs[j].plot(position_hist[0,:,j], alpha=0.5, color=colors[0]) # plot position paths beneath \n",
    "        axs[j].set(title=f'Allocations \\& Quantile: Asset {j}', xlabel='Period', ylabel=labels[j])\n",
    "        axs[j].legend()\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVaR thresholds considered:\n",
    "alphas = np.array([0.01, 0.75, 0.85, 0.99])\n",
    "\n",
    "# figure parameters\n",
    "nrows = num_assets\n",
    "ncols = len(alphas)\n",
    "fig, axes = plt.subplots(nrows, ncols, sharey='all', sharex='all', figsize=(10,7))\n",
    "\n",
    "# Instantiate one agent, then transfer-learn optimal policy for each threshold.\n",
    "agent = Agent(env, algo_params)\n",
    "\n",
    "row_titles = [r'$\\pi_{\\theta}^{(0)}$',\n",
    "              r'$\\pi_{\\theta}^{(1)}$',\n",
    "              r'$\\pi_{\\theta}^{(2)}$',\n",
    "              r'$\\pi_{\\theta}^{(3)}$']\n",
    "column_titles = [f'CVaR: {alpha}' for alpha in alphas]\n",
    "\n",
    "for alpha_idx, alpha_val in enumerate(alphas):\n",
    "    \n",
    "    agent.env.params['alpha'] = alpha_val \n",
    "    agent.Train()\n",
    "    position_hist = agent.position_history.detach().squeeze().numpy()\n",
    "\n",
    "    for idx_asset in range(num_assets):\n",
    "        \n",
    "        temp = axes[idx_asset, alpha_idx].imshow(position_hist[0][idx_asset].reshape(1,1),\n",
    "                                                 interpolation='none',\n",
    "                                                 cmap=cmap,\n",
    "                                                 aspect='auto',\n",
    "                                                 vmin=np.array(0.0),\n",
    "                                                 vmax=np.array(1.0)\n",
    "                                                )\n",
    "        axes[idx_asset, alpha_idx].set_xticks([])\n",
    "        axes[idx_asset, alpha_idx].set_yticks([])\n",
    "        \n",
    "        if alpha_idx == ncols - 1:\n",
    "            axes[idx_asset, alpha_idx].set_ylabel(row_titles[idx_asset],\n",
    "                                                  rotation='horizontal',\n",
    "                                                  fontsize=15,\n",
    "                                                  fontweight='semibold',\n",
    "                                                  labelpad=18)\n",
    "            axes[idx_asset, alpha_idx].yaxis.set_label_position(\"right\")\n",
    "\n",
    "        if idx_asset == 0:\n",
    "            axes[idx_asset, alpha_idx].set_title(column_titles[alpha_idx] + '\\n',\n",
    "                                                 rotation='horizontal',\n",
    "                                                 fontsize=15,\n",
    "                                                 fontweight='semibold')\n",
    "\n",
    "fig.colorbar(temp, ax=axes, orientation='horizontal', shrink=0.8, pad=0.1)\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Varying the Goal\n",
    "\n",
    "We investigate how the distribution of the terminal wealth evolves as the goal probability increases: $p = 0.5, 0.75, 0.80, 0.95$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goals = [0.5, 0.75, 0.80, 0.95]\n",
    "agents = []\n",
    "\n",
    "for goal in goals:\n",
    "    params = env_params\n",
    "    params[\"goal_prob\"] = goal\n",
    "    env = BS_Environment(params)\n",
    "    agent = Agent(env, algo_params)\n",
    "    agent.Train(Nsims=Nsimulations)\n",
    "    agents.append(agent)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 3, figsize=(18,5))  \\\n",
    "\n",
    "for i, agent in enumerate(agents):                        \n",
    "    # plot RDEU through training\n",
    "    ax[0].plot(np.arange(algo_params[\"num_epochs\"]) + 1, \n",
    "               T.stack(agent.RDEU_history), \n",
    "               color=colors[i],\n",
    "               label=f'p = {goals[i]}')\n",
    "    ax[0].set(xlabel='Epochs', \n",
    "              ylabel='RDEU',\n",
    "              title='RDEU by Epoch')    \n",
    "    ax[0].grid()\n",
    "\n",
    "    # plot terminal wealth distribution of learned strategy using 10,000 full episodes\n",
    "    term_wealth = agent.term_wealth_dist.detach().squeeze()\n",
    "    mean_term_wealth, std_term_wealth = T.mean(term_wealth), T.std(term_wealth)\n",
    "    num_bins = 50\n",
    "    domain = np.linspace(mean_term_wealth - 5*std_term_wealth,\n",
    "                         mean_term_wealth + 5*std_term_wealth, \n",
    "                         1500)\n",
    "    term_wealth_kde = gaussian_kde(term_wealth)\n",
    "    ax[1].hist(term_wealth, \n",
    "               density=True, \n",
    "               bins=num_bins, \n",
    "               color=colors[i], \n",
    "               alpha=0.5)\n",
    "    ax[1].set(title=r'Distribution of Terminal Wealth',\n",
    "              xlabel=r'Terminal Wealth: $X^{\\theta}$', \n",
    "              ylabel='Density')\n",
    "    ax[1].plot(domain, \n",
    "               term_wealth_kde(domain), \n",
    "               color=colors[i], \n",
    "               label=f'p = {goals[i]}')\n",
    "\n",
    "    # plot probability of posting desired return through training\n",
    "    ax[2].plot(np.arange(algo_params[\"num_epochs\"]) + 1,\n",
    "                        T.stack(agent.constraint_prob_history),\n",
    "                        color=colors[i],\n",
    "                        label=f'p = {goals[i]}')\n",
    "    ax[2].set(xlabel='Epochs', \n",
    "              ylabel=r'$\\mathbb{P}(X^\\theta \\geq (1+c)X_0)$', \n",
    "              title='Probability of Success by Epoch')    \n",
    "    ax[2].grid()\n",
    "\n",
    "    # check if constraint satisfied\n",
    "    c = env_params[\"returns_req\"]\n",
    "    print(f'Probability of return exceeding {100*c}%: {str(round(float(100*agent.return_prob), 3))}%')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
